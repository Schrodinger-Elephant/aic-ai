{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-lightning==1.2.3\n!pip install transformers\n!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:25:10.486044Z","iopub.execute_input":"2021-08-24T01:25:10.486358Z","iopub.status.idle":"2021-08-24T01:25:45.473250Z","shell.execute_reply.started":"2021-08-24T01:25:10.486288Z","shell.execute_reply":"2021-08-24T01:25:45.472330Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorch-lightning==1.2.3\n  Downloading pytorch_lightning-1.2.3-py3-none-any.whl (821 kB)\n\u001b[K     |████████████████████████████████| 821 kB 876 kB/s eta 0:00:01\n\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n\u001b[K     |████████████████████████████████| 269 kB 50.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.2.3) (4.61.1)\nRequirement already satisfied: fsspec[http]>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.2.3) (2021.6.1)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.2.3) (1.7.0)\nRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.2.3) (2.4.1)\nRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.2.3) (1.19.5)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning==1.2.3) (0.18.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (3.7.4.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (2.25.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (2.0.1)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (1.15.0)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (1.32.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (0.36.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (1.30.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (0.4.4)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (0.12.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (1.8.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (49.6.0.post20210108)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (3.17.3)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.3) (3.3.4)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (4.2.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (4.7.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (2021.5.30)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (1.26.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (4.0.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (3.1.1)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->pytorch-lightning==1.2.3) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->pytorch-lightning==1.2.3) (0.6)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (21.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (5.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (1.6.3)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.3) (3.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.3) (3.4.1)\nBuilding wheels for collected packages: PyYAML\n  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=a049cf8db518c432e741d4b4f890d4dcbfe2e5a53a812f0bfb9285721a06b555\n  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\nSuccessfully built PyYAML\nInstalling collected packages: PyYAML, pytorch-lightning\n  Attempting uninstall: PyYAML\n    Found existing installation: PyYAML 5.4.1\n    Uninstalling PyYAML-5.4.1:\n      Successfully uninstalled PyYAML-5.4.1\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 1.3.8\n    Uninstalling pytorch-lightning-1.3.8:\n      Successfully uninstalled pytorch-lightning-1.3.8\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfancyimpute 0.5.5 requires tensorflow, which is not installed.\ndask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\nfeaturetools 0.25.0 requires pyyaml>=5.4, but you have pyyaml 5.3.1 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\u001b[0m\nSuccessfully installed PyYAML-5.3.1 pytorch-lightning-1.2.3\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.6.1)\nRequirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.61.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.4.4)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.5)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt install git-lfs\n!git config --global user.email \"arkanfadhil080@gmail.com.com\"\n!git config --global user.name \"Muhammad Fadhil Arkan\"","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:20.136593Z","iopub.execute_input":"2021-08-23T17:15:20.136873Z","iopub.status.idle":"2021-08-23T17:15:26.451184Z","shell.execute_reply.started":"2021-08-23T17:15:20.136844Z","shell.execute_reply":"2021-08-23T17:15:26.450001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\n\nfrom transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:25:59.763838Z","iopub.execute_input":"2021-08-24T01:25:59.764275Z","iopub.status.idle":"2021-08-24T01:26:08.260530Z","shell.execute_reply.started":"2021-08-24T01:25:59.764229Z","shell.execute_reply":"2021-08-24T01:26:08.259718Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:34.92113Z","iopub.execute_input":"2021-08-23T17:15:34.921418Z","iopub.status.idle":"2021-08-23T17:15:34.975198Z","shell.execute_reply.started":"2021-08-23T17:15:34.921393Z","shell.execute_reply":"2021-08-23T17:15:34.974243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pth = \"./indonesian_datasets/paraphrase/paws/data/\"","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:34.978559Z","iopub.execute_input":"2021-08-23T17:15:34.97883Z","iopub.status.idle":"2021-08-23T17:15:34.984972Z","shell.execute_reply.started":"2021-08-23T17:15:34.978803Z","shell.execute_reply":"2021-08-23T17:15:34.984128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class T5FineTuner(pl.LightningModule):\n\n  def __init__(self,hparams):\n\n    # Calling the super constructer\n    super(T5FineTuner,self).__init__()\n    self.called = []\n\n    self.hparams = hparams\n\n    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n\n\n  def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n\n    return self.model(input_ids, attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=lm_labels)\n    \n  def is_logger(self):\n      return True\n    \n\n  def _step(self, batch):\n        lm_labels = batch[\"target_ids\"]\n        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n        outputs = self(\n            input_ids=batch[\"source_ids\"],\n            attention_mask=batch[\"source_mask\"],\n            lm_labels=lm_labels,\n            decoder_attention_mask=batch['target_mask']\n        )\n\n        loss = outputs[0]\n\n        return loss\n\n  def training_step(self, batch, batch_idx):\n      loss = self._step(batch)\n\n      tensorboard_logs = {\"train_loss\": loss}\n      return {\"loss\": loss, \"log\": tensorboard_logs}\n\n\n  def training_epoch_end(self, outputs):\n      avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n      tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n      return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n  def validation_step(self, batch, batch_idx):\n      loss = self._step(batch)\n      return {\"val_loss\": loss}\n\n  def validation_epoch_end(self, outputs):\n      avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n      tensorboard_logs = {\"val_loss\": avg_loss}\n      return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n\n  def configure_optimizers(self):\n    \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n    model = self.model\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": self.hparams.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n    self.opt = optimizer\n    return [optimizer]\n\n\n  def optimizer_step(\n      self,\n      epoch,\n      batch_idx,\n      optimizer,\n      optimizer_idx,\n      optimizer_closure,\n      on_tpu,\n      using_native_amp,\n      using_lbfgs,\n  ):\n      super().optimizer_step(\n          epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs\n      )\n      self.called.append(\"optimizer_step\")  # append after as closure calls other methods\n\n  def get_tqdm_dict(self):\n    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n\n    return tqdm_dict\n\n  def train_dataloader(self):\n    train_dataset = CustomDataset(tokenizer=self.tokenizer, type_path=pth+\"PAW_Train_Global\",data_dir=self.hparams.data_dir, max_len=self.hparams.max_seq_length)\n    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n                            num_workers=4)\n    t_total = (\n            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n            // self.hparams.gradient_accumulation_steps\n            * float(self.hparams.num_train_epochs)\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n    )\n    self.lr_scheduler = scheduler\n    return dataloader\n\n  def val_dataloader(self):\n    val_dataset = CustomDataset(tokenizer=self.tokenizer, type_path=pth+\"PAW_Test_Global\",data_dir=self.hparams.data_dir, max_len=self.hparams.max_seq_length)\n    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n  \n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:34.988547Z","iopub.execute_input":"2021-08-23T17:15:34.988912Z","iopub.status.idle":"2021-08-23T17:15:35.011801Z","shell.execute_reply.started":"2021-08-23T17:15:34.988873Z","shell.execute_reply":"2021-08-23T17:15:35.010965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n  def on_validation_end(self, trainer, pl_module):\n    logger.info(\"***** Validation results *****\")\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n      # Log results\n      for key in sorted(metrics):\n        if key not in [\"log\", \"progress_bar\"]:\n          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n  def on_test_end(self, trainer, pl_module):\n    logger.info(\"***** Test results *****\")\n\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n\n      # Log and save results to file\n      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n      with open(output_test_results_file, \"w\") as writer:\n        for key in sorted(metrics):\n          if key not in [\"log\", \"progress_bar\"]:\n            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:35.014476Z","iopub.execute_input":"2021-08-23T17:15:35.014988Z","iopub.status.idle":"2021-08-23T17:15:35.030937Z","shell.execute_reply.started":"2021-08-23T17:15:35.014952Z","shell.execute_reply":"2021-08-23T17:15:35.030138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyper parameters\nargs_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='panggi/t5-base-indonesian-summarization-cased',\n    tokenizer_name_or_path='panggi/t5-base-indonesian-summarization-cased',\n    max_seq_length=512,\n    learning_rate=3e-4,\n    weight_decay=0.0,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=6,\n    eval_batch_size=6,\n    num_train_epochs=2,\n    gradient_accumulation_steps=16,\n    n_gpu=1,\n    early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=42,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:35.034913Z","iopub.execute_input":"2021-08-23T17:15:35.035221Z","iopub.status.idle":"2021-08-23T17:15:35.044703Z","shell.execute_reply.started":"2021-08-23T17:15:35.035193Z","shell.execute_reply":"2021-08-23T17:15:35.0439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('panggi/t5-base-indonesian-summarization-cased')\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n        # self.path = os.path.join(data_dir, type_path + '.csv')\n\n        self.source_column = \"question1\"\n        self.target_column = \"question2\"\n        \n        self.data = []\n        \n        with open(type_path+\".csv\",\"r\") as csv_file:\n          csv_reader = csv.reader(csv_file, delimiter=',')\n          line_count = 0\n          for row in csv_reader:\n            self.data.append(row)\n\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.inputs = []\n        self.targets = []\n\n        self._build()\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n        target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n\n    def _build(self):\n        for example in self.data:\n            \n            input_ = example[0]\n            target = example[1]\n\n            input_ = \"paraphrase: \"+ input_ + ' </s>'\n            target = target + \" </s>\"\n\n            # tokenize inputs\n            tokenized_inputs = self.tokenizer.batch_encode_plus(\n                [input_], max_length=self.max_len, pad_to_max_length=True, truncation=True, return_tensors=\"pt\"\n            )\n            # tokenize targets\n            tokenized_targets = self.tokenizer.batch_encode_plus(\n                [target], max_length=self.max_len, pad_to_max_length=True,truncation=True, return_tensors=\"pt\"\n            )\n\n            self.inputs.append(tokenized_inputs)\n            self.targets.append(tokenized_targets)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:35.046485Z","iopub.execute_input":"2021-08-23T17:15:35.046936Z","iopub.status.idle":"2021-08-23T17:15:38.554467Z","shell.execute_reply.started":"2021-08-23T17:15:35.046897Z","shell.execute_reply":"2021-08-23T17:15:38.553618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Wikidepia/indonesian_datasets.git","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:38.555799Z","iopub.execute_input":"2021-08-23T17:15:38.556158Z","iopub.status.idle":"2021-08-23T17:15:43.919979Z","shell.execute_reply.started":"2021-08-23T17:15:38.556119Z","shell.execute_reply":"2021-08-23T17:15:43.918955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pth = \"./indonesian_datasets/paraphrase/paws/data/\"","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:43.921539Z","iopub.execute_input":"2021-08-23T17:15:43.921878Z","iopub.status.idle":"2021-08-23T17:15:43.926282Z","shell.execute_reply.started":"2021-08-23T17:15:43.92184Z","shell.execute_reply":"2021-08-23T17:15:43.92522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\ntrain_examples = []\ntest_examples = []\ndev_examples = []\n\nwith open(pth+\"final/train.tsv\",\"r\") as csvfile:\n\n  reader = csv.reader(csvfile,delimiter=\"\\t\")\n  \n  next(reader)\n\n  for row in reader:\n\n    if row[3] == \"1\":\n      train_examples.append((row[1],row[2]))\n\n\n\nwith open(pth+\"final/test.tsv\",\"r\") as csvfile:\n\n  reader = csv.reader(csvfile,delimiter=\"\\t\")\n  \n  next(reader)\n\n  for row in reader:\n\n    if row[3] == \"1\":\n      test_examples.append((row[1],row[2]))\n\n\nwith open(pth+\"final/dev.tsv\",\"r\") as csvfile:\n\n  reader = csv.reader(csvfile,delimiter=\"\\t\")\n  \n  next(reader)\n\n  for row in reader:\n\n    if row[3] == \"1\":\n      dev_examples.append((row[1],row[2]))\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:43.927606Z","iopub.execute_input":"2021-08-23T17:15:43.928057Z","iopub.status.idle":"2021-08-23T17:15:44.426312Z","shell.execute_reply.started":"2021-08-23T17:15:43.92802Z","shell.execute_reply":"2021-08-23T17:15:44.42546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_examples = dev_examples + test_examples","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.428526Z","iopub.execute_input":"2021-08-23T17:15:44.428907Z","iopub.status.idle":"2021-08-23T17:15:44.434967Z","shell.execute_reply.started":"2021-08-23T17:15:44.42887Z","shell.execute_reply":"2021-08-23T17:15:44.434038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_examples)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.43644Z","iopub.execute_input":"2021-08-23T17:15:44.436879Z","iopub.status.idle":"2021-08-23T17:15:44.449155Z","shell.execute_reply.started":"2021-08-23T17:15:44.436841Z","shell.execute_reply":"2021-08-23T17:15:44.448203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(pth+\"PAW_Train_Global.csv\",\"w\") as csvfile:\n  writer = csv.writer(csvfile)\n\n  for row in train_examples:\n    writer.writerow(row)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.450545Z","iopub.execute_input":"2021-08-23T17:15:44.4511Z","iopub.status.idle":"2021-08-23T17:15:44.64485Z","shell.execute_reply.started":"2021-08-23T17:15:44.451063Z","shell.execute_reply":"2021-08-23T17:15:44.643981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(pth+\"PAW_Test_Global.csv\",\"w\") as csvfile:\n  writer = csv.writer(csvfile)\n\n  for row in test_examples:\n    writer.writerow(row)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.646032Z","iopub.execute_input":"2021-08-23T17:15:44.646487Z","iopub.status.idle":"2021-08-23T17:15:44.718784Z","shell.execute_reply.started":"2021-08-23T17:15:44.646446Z","shell.execute_reply":"2021-08-23T17:15:44.717893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('t5_paw_Global'):\n    os.makedirs('t5_paw_Global')\n\nargs_dict.update({'output_dir': 't5_paw_Global','num_train_epochs':10,'max_seq_length':256})\nargs = argparse.Namespace(**args_dict)\nprint(args_dict)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.720616Z","iopub.execute_input":"2021-08-23T17:15:44.72121Z","iopub.status.idle":"2021-08-23T17:15:44.729051Z","shell.execute_reply.started":"2021-08-23T17:15:44.721167Z","shell.execute_reply":"2021-08-23T17:15:44.727879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n)\n\ntrain_params = dict(\n    accumulate_grad_batches=1,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=checkpoint_callback,\n    callbacks=[LoggingCallback()],\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.730602Z","iopub.execute_input":"2021-08-23T17:15:44.731144Z","iopub.status.idle":"2021-08-23T17:15:44.763573Z","shell.execute_reply.started":"2021-08-23T17:15:44.731104Z","shell.execute_reply":"2021-08-23T17:15:44.762832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = T5FineTuner(args)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:15:44.76483Z","iopub.execute_input":"2021-08-23T17:15:44.765363Z","iopub.status.idle":"2021-08-23T17:16:38.742427Z","shell.execute_reply.started":"2021-08-23T17:15:44.76532Z","shell.execute_reply":"2021-08-23T17:16:38.741416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\ntrainer = pl.Trainer(**train_params)\n\nprint (\" Training model\")\ntrainer.fit(model)\n\nprint (\"training finished\")\n\nprint (\"Saving model\")\nmodel.model.save_pretrained('t5_paw_global')\n\nprint (\"Saved model\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T17:16:38.743617Z","iopub.execute_input":"2021-08-23T17:16:38.743978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.model.push_to_hub(use_auth_token='api_OUmDuFxfZQolrUWWbaZXQDCaThefqqqMlB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the output\n\n\nmodel = T5ForConditionalGeneration.from_pretrained('fadhilarkan/tmpvqruuuz0')\ntokenizer = T5Tokenizer.from_pretrained('panggi/t5-base-indonesian-summarization-cased')\n\nmodel.to(\"cuda\")\n\nsentence = \"Ekonomi Neoklasik memandang ketidaksamaan dalam distribusi pendapatan sebagai timbul dari perbedaan nilai ditambahkan oleh tenaga kerja, modal, dan tanah.\"\n\ntext =  \"paraphrase: \" + sentence + \" </s>\"\n\n\nmax_len = 256\n\nencoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\n\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n# Have to read about these decodings\nbeam_outputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    do_sample=True,\n    max_length=256,\n    top_k=220,\n    top_p=1,\n    early_stopping=True,\n    num_return_sequences=5\n)\n\n\nprint (\"\\nOriginal Question ::\")\nprint (sentence)\nprint (\"\\n\")\nprint (\"Paraphrased Questions :: \")\nfinal_outputs =[]\nfor beam_output in beam_outputs:\n    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    if sent.lower() != sentence.lower() and sent not in final_outputs:\n        final_outputs.append(sent)\n\nfor i, final_output in enumerate(final_outputs):\n    print(\"{}: {}\".format(i, final_output))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:14:11.034403Z","iopub.execute_input":"2021-08-24T02:14:11.034754Z","iopub.status.idle":"2021-08-24T02:14:16.922745Z","shell.execute_reply.started":"2021-08-24T02:14:11.034724Z","shell.execute_reply":"2021-08-24T02:14:16.921858Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\nOriginal Question ::\nEkonomi Neoklasik memandang ketidaksamaan dalam distribusi pendapatan sebagai timbul dari perbedaan nilai ditambahkan oleh tenaga kerja, modal, dan tanah.\n\n\nParaphrased Questions :: \n0: Ekonomi Neoklasik, berdasarkan perbedaan nilai pada distribusi pendapatan sebagai timbul dari perbedaan nilai yang disimpan oleh tenaga kerja, modal, dan tanah.\n1: Ekonomi Neoklasik menilai ketidaksamaan dalam distribusi pendapatan sebagai bentuk dari perbedaan nilai yang ditambahkan oleh tenaga kerja, modal dan tanah.\n2: Ekonomi Neoklasik tentang ketidaksamaan dalam distribusi pendapatan sebagai hasil dari perbedaan nilai dari tenaga kerja, modal, dan tanah.\n3: Ekonomi Neoklasik memandang ketidaksamaan dalam distribusi pendapatan sebagai bagian dari perbedaan nilai yang ditambahkan oleh tenaga kerja, modal dan tanah.\n4: Ekonomi Neoklasik melihat ketidaksamaan dalam anggaran sebagai timbul dari perbedaan nilai yang ditambahkan oleh tenaga kerja, modal, dan tanah untuk kebutuhan.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nclass pipeLine_indo:\n\n    def __init__(self):\n\n        self.model_answer_name = \"fadhilarkan/qa-indo-k\"\n        self.model_question_name = \"fadhilarkan/gq-indo-k\"\n        self.model_paraphrase = T5ForConditionalGeneration.from_pretrained('fadhilarkan/tmpvqruuuz0')\n        self.model_paraphrase_token = T5Tokenizer.from_pretrained('panggi/t5-base-indonesian-summarization-cased')\n        self.paraphrase_maxlen = 256\n        \n        self.nlp_answer = pipeline('question-answering', model=self.model_answer_name, tokenizer=self.model_answer_name)\n        self.nlp_question = pipeline('text2text-generation', model=self.model_question_name, tokenizer=self.model_question_name)\n                                     \n    def generate_question(self,context):\n\n        input = str(context)\n\n        question = self.nlp_question(input)\n\n        return question[0]['generated_text']\n\n    def predict_answer(self,context,question):\n\n        input = {'question': question,\n                 'context': context\n                }\n\n        answer = self.nlp_answer(input)\n\n        return answer['answer']\n\n    def paraphrase(self,sentence):\n\n        text =  \"paraphrase: \" + sentence + \" </s>\"\n        encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n        input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n        beam_outputs = model.generate(input_ids=input_ids, attention_mask=attention_masks,\n                                      do_sample=True,max_length=256,top_k=220,top_p=1,\n                                      early_stopping=True,num_return_sequences=5\n                                     )\n        \n        final_outputs =[]\n        for beam_output in beam_outputs:\n            sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n            if sent.lower() != sentence.lower() and sent not in final_outputs:\n                final_outputs.append(sent)\n                \n        return final_outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:32:48.214945Z","iopub.execute_input":"2021-08-24T02:32:48.215276Z","iopub.status.idle":"2021-08-24T02:32:48.227143Z","shell.execute_reply.started":"2021-08-24T02:32:48.215246Z","shell.execute_reply":"2021-08-24T02:32:48.226334Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"a = pipeLine_indo()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:32:59.539040Z","iopub.execute_input":"2021-08-24T02:32:59.539359Z","iopub.status.idle":"2021-08-24T02:33:11.134712Z","shell.execute_reply.started":"2021-08-24T02:32:59.539329Z","shell.execute_reply":"2021-08-24T02:33:11.133847Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}